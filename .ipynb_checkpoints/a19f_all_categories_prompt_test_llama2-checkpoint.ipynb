{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c77629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "593e2b6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m llm \u001b[38;5;241m=\u001b[39m Ollama(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama2:text\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m             temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      3\u001b[0m             system\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput your response in proper JSON format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe first man on the moon was ...\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/langchain_core/language_models/llms.py:230\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    227\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    228\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    241\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/langchain_core/language_models/llms.py:525\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    519\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    523\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    524\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/langchain_core/language_models/llms.py:698\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    683\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m         )\n\u001b[1;32m    685\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    686\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    687\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    696\u001b[0m         )\n\u001b[1;32m    697\u001b[0m     ]\n\u001b[0;32m--> 698\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/langchain_core/language_models/llms.py:562\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    561\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 562\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    563\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/langchain_core/language_models/llms.py:549\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    541\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    546\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 549\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    553\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    556\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    557\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    558\u001b[0m         )\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/langchain_community/llms/ollama.py:389\u001b[0m, in \u001b[0;36mOllama._generate\u001b[0;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m--> 389\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/langchain_community/llms/ollama.py:298\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    291\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    296\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[1;32m    297\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[1;32m    300\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/langchain_community/llms/ollama.py:148\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[0;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    142\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    146\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    147\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[1;32m    149\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[1;32m    150\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    151\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    153\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/requests/models.py:865\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(\n\u001b[1;32m    866\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, decode_unicode\u001b[38;5;241m=\u001b[39mdecode_unicode\n\u001b[1;32m    867\u001b[0m ):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    870\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m pending \u001b[38;5;241m+\u001b[39m chunk\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/requests/utils.py:571\u001b[0m, in \u001b[0;36mstream_decode_response_unicode\u001b[0;34m(iterator, r)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    570\u001b[0m decoder \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetincrementaldecoder(r\u001b[38;5;241m.\u001b[39mencoding)(errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m    572\u001b[0m     rv \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(chunk)\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rv:\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/urllib3/response.py:624\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 624\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content):\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m line\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/urllib3/response.py:828\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    830\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/urllib3/response.py:758\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama2:text\",\n",
    "            temperature=0,\n",
    "            system=\"output your response in proper JSON format\")\n",
    "\n",
    "\n",
    "llm.invoke(\"The first man on the moon was ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96d96b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/chrissoria/Documents/Research/Categorization_AI_experiments')\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7b7a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_participant_input = \"a19f\" #enter column name here\n",
    "UCNets = pd.read_excel(\"../UCNets_Classification/Hand_Coding_Surveys/a19fg/a19fg_Master.xlsx\", engine='openpyxl', sheet_name=\"master_a19f\")\n",
    "\n",
    "UCNets = UCNets[['Response']]\n",
    "UCNets.rename(columns={'Response': 'a19f'}, inplace=True)\n",
    "\n",
    "UCNets = UCNets[survey_participant_input].dropna().unique()  # Drop NaN values and get unique elements\n",
    "\n",
    "survey_participant_responses = '; '.join(str(item) for item in UCNets) #what we will feed to the model\n",
    "\n",
    "UCNets = pd.DataFrame(UCNets, columns=[survey_participant_input])\n",
    "UCNets[survey_participant_input] = UCNets[survey_participant_input].astype(str).str.lower()\n",
    "UCNets[survey_participant_input] = UCNets[survey_participant_input].str.strip()\n",
    "UCNets = UCNets[UCNets[survey_participant_input] != ''].reset_index(drop=True) #trimming all empty rows\n",
    "\n",
    "UCNets = UCNets.iloc[:400]\n",
    "\n",
    "UCNets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b77e00c",
   "metadata": {},
   "source": [
    "Here, I'm trying to \"force\" the model to \"think\" in steps by first A. trying to process the response into its own words and B. having it interact with that object. That is, instead of all steps being given at once, I'm having it think in steps. \n",
    "\n",
    "This time, I will have it think in a \"chain,\" where I will have it output a response and then feed that response back to it in a seperate prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f0b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_categories(survey_question, \n",
    "                       survey_input,\n",
    "                       user_model,\n",
    "                       creativity,\n",
    "                       categories):\n",
    "    \n",
    "    categories_str = \"\\n\".join(f\"{i + 1}. {cat}\" for i, cat in enumerate(categories))\n",
    "    cat_num = len(user_categories)\n",
    "    category_dict = {str(i+1): 0 for i in range(cat_num)}\n",
    "    example_JSON = json.dumps(category_dict, indent=4)\n",
    "    print(example_JSON)\n",
    "    llm = Ollama(model=user_model,\n",
    "        temperature=creativity,\n",
    "            system=f\"output your response in this desired JSON format where each category number is the key: {example_JSON}\")\n",
    "    \n",
    "    link1 = []\n",
    "    extracted_jsons = []\n",
    "    \n",
    "    for response in survey_input:\n",
    "        prompt = f\"\"\"Categorize this survey response \"{response}\" into all of the following ways they took steps to make new friends and select all that apply: \\\n",
    "        {categories_str} \\\n",
    "        Provide your work in the JSON format where the number belonging to each category is the key and a 1 if the category is present and a 0 if it is not present as key values.\"\"\"\n",
    "        try:\n",
    "            print(prompt)\n",
    "            reply = llm.invoke(prompt)\n",
    "            \n",
    "            link1.append(reply)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            link1.append(f\"Error processing input: {input}\")\n",
    "            \n",
    "        extracted_json = re.findall(r'(\\{.*?\\})', reply, re.DOTALL)\n",
    "        extracted_json = ''.join(extracted_json)\n",
    "\n",
    "        print(extracted_json)\n",
    "        if extracted_json:\n",
    "            cleaned_json = extracted_json.replace('[', '').replace(']', '').replace('\\n', '').replace(\" \", '').replace(\"  \", '')\n",
    "            extracted_jsons.append(cleaned_json)\n",
    "            print(cleaned_json)\n",
    "        else:\n",
    "            error_message = \"\"\"{\"1\":\"e\"}\"\"\"\n",
    "            extracted_jsons.append(error_message)\n",
    "            print(error_message)\n",
    "            \n",
    "    normalized_data_list = []\n",
    "    error_lines = []\n",
    "    \n",
    "    for i, json_str in enumerate(extracted_jsons):\n",
    "        try:\n",
    "            # Attempt to parse the JSON string\n",
    "            parsed_obj = json.loads(json_str)\n",
    "            # Convert the parsed object to a DataFrame and append\n",
    "            normalized_data_list.append(pd.json_normalize(parsed_obj))\n",
    "        except json.JSONDecodeError:\n",
    "            # Define a default JSON object as a dictionary\n",
    "            default_json_obj = {\"1\": \"j\"}\n",
    "            # Convert the default object to a DataFrame and append\n",
    "            normalized_data_list.append(pd.json_normalize(default_json_obj))\n",
    "\n",
    "    normalized_data = pd.concat(normalized_data_list, ignore_index=True)\n",
    "    \n",
    "    categorized_data = pd.DataFrame()\n",
    "    categorized_data['survey_response'] = survey_input\n",
    "    categorized_data['link1'] = link1\n",
    "    categorized_data['json'] = extracted_jsons\n",
    "    \n",
    "    categorized_data = pd.concat([categorized_data, normalized_data], axis=1)\n",
    "    \n",
    "    return categorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae55712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_question = \"After this last move, what steps, if any, did you take in order to make new friends?\"\n",
    "survey_input = UCNets['a19f']\n",
    "\n",
    "creativity = .0\n",
    "\n",
    "user_categories = [\"Engaged with local religious institutions such as churches, synagogues, mosques, or other forms of religious communities.\",\n",
    "                   \"Frequented local establishments like bars, cafes, shops, or malls to interact with individuals present in the vicinity.\",\n",
    "                   \"Direct involvement in secular volunteering efforts, contributing through action and service rather than mere membership in volunteer groups.\",\n",
    "                   \"Utilizing digital platforms such as online chats, internet networking websites, or dating apps to establish connections and friendships.\",\n",
    "                   \"Engaged in informal, non-professional interactions and outings with colleagues to foster friendships.\",\n",
    "                   \"Involvement in sports, exercise, or outdoor recreational activities through gyms, teams, or athletic clubs.\",\n",
    "                   \"Participated in organizations or taking recreational classes related to arts, books, music, theater, crafts, or similar cultural and hobby pursuits.\",\n",
    "                   \"Initiating contact with neighbors, participating in neighborhood or local community groups.\",\n",
    "                   \"Engaged in a broad array of in-person events or groups that do not fit into any of the other categories. These could include parties, ethnic activities, political canvassing, festivals, school activities, or senior centers.\"]\n",
    "\n",
    "user_model = 'llama2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b40959",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bad = extract_categories(survey_question, \n",
    "                            survey_input, \n",
    "                            user_model,\n",
    "                            creativity,\n",
    "                            user_categories)\n",
    "\n",
    "bad.to_csv('data/a19f_bad_categorization_5_cats_llama.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f7c8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a26cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_categories_improved(survey_question, \n",
    "                       survey_input,\n",
    "                       user_model,\n",
    "                       creativity,\n",
    "                       categories):\n",
    "    \n",
    "    categories_str = \"\\n\".join(f\"{i + 1}. {cat}\" for i, cat in enumerate(categories))\n",
    "    cat_num = len(user_categories)\n",
    "    category_dict = {str(i+1): \"0\" for i in range(cat_num)}\n",
    "    example_JSON = json.dumps(category_dict, indent=4)\n",
    "    print(example_JSON)\n",
    "    llm = Ollama(model=user_model,\n",
    "        temperature=creativity,\n",
    "            system=f\"\"\"You are an expert in identifying themes and patterns in open-ended survey responses to the question, \"{survey_question}\". \\\n",
    "                      When given a survey response, you analyze it critically and thoroughly to identify user-provided categories present in the response. \\\n",
    "                      Output your response in this desired format: {example_JSON}\"\"\")\n",
    "    \n",
    "    link1 = []\n",
    "    extracted_jsons = []\n",
    "    \n",
    "    for response in survey_input:\n",
    "        prompt = f\"\"\"A survey respondent was asked, \"{survey_question}\". \\\n",
    "        Their response is here in triple backticks: ```{response}```. \\\n",
    "        Select all of the following numbered categories present in the response and form your response in proper JSON format: \\\n",
    "        The number belonging to the category should be be the key and a 1 is the key value if the category is present. \\\n",
    "        If none of the categories are present in their response, provide 0's for all key values in your JSON. \\\n",
    "        Numbered categories: \"{categories_str}\".\"\"\"\n",
    "        print(prompt)\n",
    "        try:\n",
    "            reply = llm.invoke(prompt)\n",
    "            \n",
    "            link1.append(reply)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            link1.append(f\"Error processing input: {input}\")\n",
    "            \n",
    "        extracted_json = re.findall(r'(\\{.*?\\})', reply, re.DOTALL)\n",
    "        extracted_json = ''.join(extracted_json)\n",
    "\n",
    "        print(extracted_json)\n",
    "        if extracted_json:\n",
    "            cleaned_json = extracted_json.replace('[', '').replace(']', '').replace('\\n', '').replace(\" \", '').replace(\"  \", '')\n",
    "            extracted_jsons.append(cleaned_json)\n",
    "            print(cleaned_json)\n",
    "        else:\n",
    "            error_message = \"\"\"{\"1\":\"e\"}\"\"\"\n",
    "            extracted_jsons.append(error_message)\n",
    "            print(error_message)\n",
    "            \n",
    "    normalized_data_list = []\n",
    "    error_lines = []\n",
    "    \n",
    "    for i, json_str in enumerate(extracted_jsons):\n",
    "        try:\n",
    "            # Attempt to parse the JSON string\n",
    "            parsed_obj = json.loads(json_str)\n",
    "            # Convert the parsed object to a DataFrame and append\n",
    "            normalized_data_list.append(pd.json_normalize(parsed_obj))\n",
    "        except json.JSONDecodeError:\n",
    "            # Define a default JSON object as a dictionary\n",
    "            default_json_obj = {\"1\": \"j\"}\n",
    "            # Convert the default object to a DataFrame and append\n",
    "            normalized_data_list.append(pd.json_normalize(default_json_obj))\n",
    "\n",
    "    normalized_data = pd.concat(normalized_data_list, ignore_index=True)\n",
    "    \n",
    "    categorized_data = pd.DataFrame()\n",
    "    categorized_data['survey_response'] = survey_input\n",
    "    categorized_data['link1'] = link1\n",
    "    categorized_data['json'] = extracted_jsons\n",
    "    \n",
    "    categorized_data = pd.concat([categorized_data, normalized_data], axis=1)\n",
    "    \n",
    "    return categorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7d2098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "good = extract_categories_improved(survey_question, \n",
    "                            survey_input, \n",
    "                            user_model,\n",
    "                            creativity,\n",
    "                            user_categories)\n",
    "\n",
    "good.to_csv('data/a19f_good_categorization_5_cats_llama.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0005c706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_categories_cot(survey_question, \n",
    "                       survey_input,\n",
    "                       user_model,\n",
    "                       creativity,\n",
    "                       categories):\n",
    "    \n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    categories_str = \"\\n\".join(f\"{i + 1}. {cat}\" for i, cat in enumerate(categories))\n",
    "    cat_num = len(user_categories)\n",
    "    category_dict = {str(i+1): \"0\" for i in range(cat_num)}\n",
    "    example_JSON = json.dumps(category_dict, indent=4)\n",
    "    \n",
    "    link1 = []\n",
    "    extracted_jsons = []\n",
    "    \n",
    "    for response in survey_input:\n",
    "        prompt = f\"\"\"A survey respondent was asked, \"{survey_question}\" \\\n",
    "        Their response is here in triple backticks: ```{response}```. \\\n",
    "        First, thoruoughly extract all their answers to the question and be as specific as possible. \\\n",
    "        Second, take these reasons and select all of the following numbered categories they fall into: \\\n",
    "        \"{categories_str}\" \\\n",
    "        Third, form your response in proper JSON format. \\\n",
    "        The number belonging to the category shoulbe be the key and a 1 is the key value if the category is present. \\\n",
    "        If none of the categories are present in their response, provide 0's for all key values in your JSON.\"\"\"\n",
    "        print(prompt)\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=user_model,\n",
    "                messages=[\n",
    "                    {\n",
    "                      \"role\": \"system\",\n",
    "                      \"content\": f\"\"\"You are an expert in identifying themes and patterns in open-ended survey responses to the question, \"{survey_question}\". \\\n",
    "                      When given a survey response, you analyze it critically and thoroughly to identify user-provided categories present in the response.\"\"\"\n",
    "                    },\n",
    "                    {'role': 'user', \n",
    "                     'content': prompt}\n",
    "                ],\n",
    "                temperature=creativity\n",
    "            )\n",
    "\n",
    "            reply = response.choices[0].message.content\n",
    "            link1.append(reply)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            link1.append(f\"Error processing input: {input}\")\n",
    "            \n",
    "        extracted_json = re.findall(r'```json\\n(\\{.*?\\})\\n```', reply, re.DOTALL)\n",
    "            \n",
    "        if extracted_json:\n",
    "            cleaned_json = extracted_json[0].replace('[', '').replace(']', '').replace('\\n', '').replace(\" \", '').replace(\"  \", '')\n",
    "            extracted_jsons.append(cleaned_json)\n",
    "            print(cleaned_json)\n",
    "        else:\n",
    "            error_message = \"\"\"{\"1\":\"e\"}\"\"\"\n",
    "            extracted_jsons.append(error_message)\n",
    "            print(error_message)\n",
    "            \n",
    "    normalized_data_list = []\n",
    "    error_lines = []\n",
    "    \n",
    "    for i, json_str in enumerate(extracted_jsons):\n",
    "        try:\n",
    "            parsed_obj = json.loads(json_str)\n",
    "            normalized_data_list.append(pd.json_normalize(parsed_obj))\n",
    "        except json.JSONDecodeError:\n",
    "            normalized_data_list.append(\"\"\"{\"1\":\"e\"}\"\"\")\n",
    "            continue\n",
    "\n",
    "    normalized_data = pd.concat(normalized_data_list, ignore_index=True)\n",
    "    \n",
    "    categorized_data = pd.DataFrame()\n",
    "    categorized_data['survey_response'] = survey_input\n",
    "    categorized_data['link1'] = link1\n",
    "    categorized_data['json'] = extracted_jsons\n",
    "    \n",
    "    categorized_data = pd.concat([categorized_data, normalized_data], axis=1)\n",
    "    \n",
    "    return categorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445db412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cot = extract_categories_cot(survey_question, \n",
    "                            survey_input, \n",
    "                            user_model,\n",
    "                            creativity,\n",
    "                            user_categories)\n",
    "\n",
    "cot.to_csv('data/a19f_cot_categorization_5_cats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1aaee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_categories_cove(survey_question, \n",
    "                       survey_input,\n",
    "                       user_model,\n",
    "                       creativity,\n",
    "                       categories):\n",
    "    \n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    categories_str = \"\\n\".join(f\"{i + 1}. {cat}\" for i, cat in enumerate(categories))\n",
    "    cat_num = len(categories)\n",
    "    category_dict = {str(i+1): \"0\" for i in range(cat_num)}\n",
    "    example_JSON = json.dumps(category_dict, indent=4)\n",
    "    \n",
    "    link1 = []\n",
    "    link2 = []\n",
    "    extracted_jsons = []\n",
    "    \n",
    "    for response in survey_input:\n",
    "        prompt = f\"\"\"Categorize this survey response \"{response}\" into all of the following reasons for moving and select all that apply: \\\n",
    "        {categories_str} \\\n",
    "        Provide your work in JSON format where the number belonging to each category is the key and a 1 if the category is present and a 0 if it is not present as key values.\"\"\"\n",
    "        try:\n",
    "            api_response = client.chat.completions.create(\n",
    "                model=user_model,\n",
    "                messages=[\n",
    "                    {'role': 'user', \n",
    "                     'content': prompt}\n",
    "                ],\n",
    "                temperature=creativity\n",
    "            )\n",
    "            reply = api_response.choices[0].message.content\n",
    "            print(reply)\n",
    "            link1.append(reply)\n",
    "\n",
    "            prompt2 = f\"\"\"Thank you for categorizing this survey response, \"{response}\". \\\n",
    "            Can you double check if there are any categories you might've missed or marked as being present incorrectly? \\\n",
    "            Here are the categories once again: {categories_str} \\\n",
    "            If there are any changes, please output a corrected JSON with the new categorization. \\\n",
    "            If there are no changes, please output the original JSON.\"\"\"\n",
    "            print(prompt2)\n",
    "            \n",
    "\n",
    "            api_response2 = client.chat.completions.create(\n",
    "                model=user_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": f\"\"\"You just categorized an answer to the question, \"{survey_question}\". You are revisiting your initial categorizations for accuracy. Here's what you initially identified: {reply}\"\"\"}, \n",
    "                    {'role':'assistant', 'content': reply},\n",
    "                    {'role': 'user', 'content': prompt2}\n",
    "                ],\n",
    "                temperature=.25,\n",
    "            )\n",
    "\n",
    "            reply2 = api_response2.choices[0].message.content\n",
    "            link2.append(reply2)\n",
    "            \n",
    "        extracted_json = re.findall(r'```json\\n(\\{.*?\\})\\n```', reply, re.DOTALL)\n",
    "            \n",
    "        if extracted_json:\n",
    "            cleaned_json = extracted_json[0].replace('[', '').replace(']', '').replace('\\n', '').replace(\" \", '').replace(\"  \", '')\n",
    "            extracted_jsons.append(cleaned_json)\n",
    "            print(cleaned_json)\n",
    "        else:\n",
    "            error_message = \"\"\"{\"1\":\"e\"}\"\"\"\n",
    "            extracted_jsons.append(error_message)\n",
    "            print(error_message)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            link1.append(f\"Error processing input: {survey_input}\")\n",
    "            link2.append(f\"Error processing response: {reply}\")\n",
    "            \n",
    "    normalized_data_list = []\n",
    "    error_lines = []\n",
    "    \n",
    "    for i, json_str in enumerate(extracted_jsons):\n",
    "        try:\n",
    "            parsed_obj = json.loads(json_str)\n",
    "            normalized_data_list.append(pd.json_normalize(parsed_obj))\n",
    "        except json.JSONDecodeError:\n",
    "            normalized_data_list.append(\"\"\"{\"1\":\"e\"}\"\"\")\n",
    "            continue\n",
    "\n",
    "    normalized_data = pd.concat(normalized_data_list, ignore_index=True)\n",
    "    \n",
    "    categorized_data = pd.DataFrame()\n",
    "    categorized_data['survey_response'] = survey_input\n",
    "    categorized_data['link1'] = link1\n",
    "    categorized_data['link2'] = link2\n",
    "    categorized_data['json'] = extracted_jsons\n",
    "    \n",
    "    categorized_data = pd.concat([categorized_data, normalized_data], axis=1)\n",
    "    \n",
    "    return categorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba270ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "creativity = .0\n",
    "\n",
    "cove = extract_categories_cove(survey_question, \n",
    "                            survey_input, \n",
    "                            user_model,\n",
    "                            creativity,\n",
    "                            user_categories)\n",
    "\n",
    "cove.to_csv('data/a19f_cove_categorization_5_cats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e5fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_categories_1s(survey_question, \n",
    "                       survey_input,\n",
    "                       user_model,\n",
    "                       creativity,\n",
    "                       categories):\n",
    "    \n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    categories_str = \"\\n\".join(f\"{i + 1}. {cat}\" for i, cat in enumerate(categories))\n",
    "    cat_num = len(user_categories)\n",
    "    category_dict = {str(i+1): \"0\" for i in range(cat_num)}\n",
    "    example_categorization = \"\"\"{\"1\":\"0\",\"2\":\"0\",\"3\":\"0\",\"4\":\"0\",\"5\":\"1\",\"6\":\"0\",\"7\":\"1\",\"8\":\"0\",\"9\":\"1\"}\"\"\"\n",
    "    #1. going out with coworkers, 2. board games (cultural-hobby), 3. activities in the city (broader participation)\n",
    "    example_response = \"going out with coworkers, setting up events with friends and having them invite anyone they wanted, participating in game nights (board games) which tend to have different attendees each night, activities in the city.\"\n",
    "    link1 = []\n",
    "    extracted_jsons = []\n",
    "    \n",
    "    for response in survey_input:\n",
    "        prompt = f\"\"\"Categorize this survey response \"{response}\" into all of the following reasons for moving and select all that apply: \\\n",
    "        {categories_str} \\\n",
    "        Provide your work in JSON format where the number belonging to each category is the key and a 1 if the category is present and a 0 if it is not present as key values. \\\n",
    "        Here's an example of a correct categorization. \\\n",
    "        Example survey response: {example_response}. \\\n",
    "        Example categorization: {example_categorization}.\"\"\"\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=user_model,\n",
    "                messages=[\n",
    "                    {'role': 'user', 'content': prompt}\n",
    "                ],\n",
    "                temperature=creativity\n",
    "            )\n",
    "\n",
    "            reply = response.choices[0].message.content\n",
    "            link1.append(reply)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            link1.append(f\"Error processing input: {input}\")\n",
    "           \n",
    "        extracted_json = re.findall(r'```json\\n(\\{.*?\\})\\n```', reply, re.DOTALL)\n",
    "            \n",
    "        if extracted_json:\n",
    "            cleaned_json = extracted_json[0].replace('[', '').replace(']', '').replace('\\n', '').replace(\" \", '').replace(\"  \", '')\n",
    "            extracted_jsons.append(cleaned_json)\n",
    "            print(cleaned_json)\n",
    "        else:\n",
    "            error_message = \"\"\"{\"1\":\"e\"}\"\"\"\n",
    "            extracted_jsons.append(error_message)\n",
    "            print(error_message)\n",
    "            \n",
    "    normalized_data_list = []\n",
    "    error_lines = []\n",
    "    \n",
    "    for i, json_str in enumerate(extracted_jsons):\n",
    "        try:\n",
    "            parsed_obj = json.loads(json_str)\n",
    "            normalized_data_list.append(pd.json_normalize(parsed_obj))\n",
    "        except json.JSONDecodeError:\n",
    "            normalized_data_list.append(\"\"\"{\"1\":\"e\"}\"\"\")\n",
    "            continue\n",
    "\n",
    "    normalized_data = pd.concat(normalized_data_list, ignore_index=True)\n",
    "    \n",
    "    categorized_data = pd.DataFrame()\n",
    "    categorized_data['survey_response'] = survey_input\n",
    "    categorized_data['link1'] = link1\n",
    "    categorized_data['json'] = extracted_jsons\n",
    "    \n",
    "    categorized_data = pd.concat([categorized_data, normalized_data], axis=1)\n",
    "    \n",
    "    return categorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75a914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "oneshot = extract_categories_1s(survey_question, \n",
    "                            survey_input, \n",
    "                            user_model,\n",
    "                            creativity,\n",
    "                            user_categories)\n",
    "\n",
    "oneshot.to_csv('data/a19f_1s_categorization_5_cats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b29e8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_categories_fs(survey_question, \n",
    "                       survey_input,\n",
    "                       user_model,\n",
    "                       creativity,\n",
    "                       categories):\n",
    "    \n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    categories_str = \"\\n\".join(f\"{i + 1}. {cat}\" for i, cat in enumerate(categories))\n",
    "    cat_num = len(user_categories)\n",
    "    category_dict = {str(i+1): \"0\" for i in range(cat_num)}\n",
    "    example_response = \"the rent was increasing; they wanted to renew the rent $400 extra, a month to re-lease. made sense regarding my career. and i wanted a backyard for my dog.\"\n",
    "    example_categorization = \"\"\"{\"1\":\"0\",\"2\":\"1\",\"3\":\"0\",\"4\":\"1\",\"5\":\"0\"}\"\"\"\n",
    "    example_response2 = \"lease ended at my old apartment and i wanted to move back to my parents house to pay off more of my student loans\"\n",
    "    example_categorization2 = \"\"\"{\"1\":\"0\",\"2\":\"0\",\"3\":\"0\",\"4\":\"1\",\"5\":\"1\"}\"\"\"\n",
    "    example_response3 = \"there was a fire in building where i previously lived; all tenants displaced, we had to find other housing. after the fire i stayed 3 days with a friend, then 2 months in a hotel, then began living in my current apartment in same city. move was not by choice, was circumstantial.\"\n",
    "    example_categorization3 = \"\"\"{\"1\":\"0\",\"2\":\"0\",\"3\":\"0\",\"4\":\"0\",\"5\":\"0\"}\"\"\"\n",
    "    \n",
    "    link1 = []\n",
    "    extracted_jsons = []\n",
    "    \n",
    "    for response in survey_input:\n",
    "        prompt = f\"\"\"Categorize this survey response \"{response}\" into all of the following reasons for moving and select all that apply: \\\n",
    "        {categories_str} \\\n",
    "        Provide your work in JSON format where the number belonging to each category is the key and a 1 if the category is present and a 0 if it is not present as key values. \\\n",
    "        Here are three examples of a correct categorization. \\\n",
    "        Example survey response 1: {example_response}. \\\n",
    "        Example categorization 1: {example_categorization}. \\\n",
    "        Example survey response 2: {example_response2}. \\\n",
    "        Example categorization 2: {example_categorization2}. \\\n",
    "        Example survey response 3: {example_response3}. \\\n",
    "        Example categorization 3: {example_categorization3}.\"\"\"\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=user_model,\n",
    "                messages=[\n",
    "                    {'role': 'user', 'content': prompt}\n",
    "                ],\n",
    "                temperature=creativity\n",
    "            )\n",
    "\n",
    "            reply = response.choices[0].message.content\n",
    "            link1.append(reply)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            link1.append(f\"Error processing input: {input}\")\n",
    "            \n",
    "        extracted_json = re.findall(r'```json\\n(\\{.*?\\})\\n```', reply, re.DOTALL)\n",
    "            \n",
    "        if extracted_json:\n",
    "            cleaned_json = extracted_json[0].replace('[', '').replace(']', '').replace('\\n', '').replace(\" \", '').replace(\"  \", '')\n",
    "            extracted_jsons.append(cleaned_json)\n",
    "            print(cleaned_json)\n",
    "        else:\n",
    "            error_message = \"\"\"{\"1\":\"e\"}\"\"\"\n",
    "            extracted_jsons.append(error_message)\n",
    "            print(error_message)\n",
    "            \n",
    "    normalized_data_list = []\n",
    "    error_lines = []\n",
    "    \n",
    "    for i, json_str in enumerate(extracted_jsons):\n",
    "        try:\n",
    "            parsed_obj = json.loads(json_str)\n",
    "            normalized_data_list.append(pd.json_normalize(parsed_obj))\n",
    "        except json.JSONDecodeError:\n",
    "            normalized_data_list.append(\"\"\"{\"1\":\"e\"}\"\"\")\n",
    "            continue\n",
    "\n",
    "    normalized_data = pd.concat(normalized_data_list, ignore_index=True)\n",
    "    \n",
    "    categorized_data = pd.DataFrame()\n",
    "    categorized_data['survey_response'] = survey_input\n",
    "    categorized_data['link1'] = link1\n",
    "    categorized_data['json'] = extracted_jsons\n",
    "    \n",
    "    categorized_data = pd.concat([categorized_data, normalized_data], axis=1)\n",
    "    \n",
    "    return categorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dccc6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fewshot = extract_categories_fs(survey_question, \n",
    "                            survey_input, \n",
    "                            user_model,\n",
    "                            creativity,\n",
    "                            user_categories)\n",
    "\n",
    "fewshot.to_csv('data/a19i_fs_categorization_5_cats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb51e88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“AI”",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
