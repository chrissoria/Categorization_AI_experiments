
@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	urldate = {2024-02-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	year = {2020},
	pages = {1877--1901},
	file = {Full Text PDF:/Users/chrissoria/Zotero/storage/HKEGYVCH/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@misc{mccoy_right_2019,
	title = {Right for the {Wrong} {Reasons}: {Diagnosing} {Syntactic} {Heuristics} in {Natural} {Language} {Inference}},
	shorttitle = {Right for the {Wrong} {Reasons}},
	url = {http://arxiv.org/abs/1902.01007},
	doi = {10.48550/arXiv.1902.01007},
	abstract = {A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area},
	urldate = {2024-02-02},
	publisher = {arXiv},
	author = {McCoy, R. Thomas and Pavlick, Ellie and Linzen, Tal},
	month = jun,
	year = {2019},
	note = {arXiv:1902.01007 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Camera-ready for ACL 2019},
	file = {arXiv Fulltext PDF:/Users/chrissoria/Zotero/storage/W3KX7XLS/McCoy et al. - 2019 - Right for the Wrong Reasons Diagnosing Syntactic .pdf:application/pdf;arXiv.org Snapshot:/Users/chrissoria/Zotero/storage/NYE7FVXD/1902.html:text/html},
}

@inproceedings{radford_language_2019,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	url = {https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	urldate = {2024-02-02},
	author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, D. and Amodei, Dario and Sutskever, I.},
	year = {2019},
	annote = {[TLDR] It is demonstrated that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText, suggesting a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	file = {Full Text PDF:/Users/chrissoria/Zotero/storage/AQXJTCBF/Radford et al. - 2019 - Language Models are Unsupervised Multitask Learner.pdf:application/pdf},
}

@misc{ziegler_fine-tuning_2020,
	title = {Fine-{Tuning} {Language} {Models} from {Human} {Preferences}},
	url = {http://arxiv.org/abs/1909.08593},
	doi = {10.48550/arXiv.1909.08593},
	abstract = {Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.},
	urldate = {2024-02-02},
	publisher = {arXiv},
	author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
	month = jan,
	year = {2020},
	note = {arXiv:1909.08593 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/chrissoria/Zotero/storage/WQSWSNDR/Ziegler et al. - 2020 - Fine-Tuning Language Models from Human Preferences.pdf:application/pdf;arXiv.org Snapshot:/Users/chrissoria/Zotero/storage/KJ272Z5F/1909.html:text/html},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
	urldate = {2024-02-02},
	publisher = {arXiv},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	month = jan,
	year = {2023},
	note = {arXiv:2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/chrissoria/Zotero/storage/8UKMKDGZ/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf;arXiv.org Snapshot:/Users/chrissoria/Zotero/storage/DALX75XI/2201.html:text/html},
}

@inproceedings{ling_program_2017,
	address = {Vancouver, Canada},
	title = {Program {Induction} by {Rationale} {Generation}: {Learning} to {Solve} and {Explain} {Algebraic} {Word} {Problems}},
	shorttitle = {Program {Induction} by {Rationale} {Generation}},
	url = {https://aclanthology.org/P17-1015},
	doi = {10.18653/v1/P17-1015},
	abstract = {Solving algebraic word problems requires executing a series of arithmetic operations—a program—to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.},
	urldate = {2024-02-03},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
	editor = {Barzilay, Regina and Kan, Min-Yen},
	month = jul,
	year = {2017},
	pages = {158--167},
	file = {Full Text PDF:/Users/chrissoria/Zotero/storage/GFW2737V/Ling et al. - 2017 - Program Induction by Rationale Generation Learnin.pdf:application/pdf},
}

@article{li_structured_2023,
	title = {Structured {Chain}-of-{Thought} {Prompting} for {Code} {Generation}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2305.06599},
	doi = {10.48550/ARXIV.2305.06599},
	abstract = {Large Language Models (LLMs) (e.g., ChatGPT) have shown impressive performance in code generation. LLMs take prompts as inputs, and Chain-of-Thought (CoT) prompting is the state-of-the-art prompting technique. CoT prompting asks LLMs first to generate CoTs (i.e., intermediate natural language reasoning steps) and then output the code. However, CoT prompting is designed for natural language generation and has low accuracy in code generation. In this paper, we propose Structured CoTs (SCoTs) and present a novel prompting technique for code generation, named SCoT prompting. Our motivation is source code contains rich structural information and any code can be composed of three program structures (i.e., sequence, branch, and loop structures). Intuitively, structured intermediate reasoning steps make for structured source code. Thus, we ask LLMs to use program structures to build CoTs, obtaining SCoTs. Then, LLMs generate the final code based on SCoTs. Compared to CoT prompting, SCoT prompting explicitly constrains LLMs to think about how to solve requirements from the view of source code and further the performance of LLMs in code generation. We apply SCoT prompting to two LLMs (i.e., ChatGPT and Codex) and evaluate it on three benchmarks (i.e., HumanEval, MBPP, and MBCPP). (1) SCoT prompting outperforms the state-of-the-art baseline - CoT prompting by up to 13.79\% in Pass@1. (2) Human evaluation shows human developers prefer programs from SCoT prompting. (3) SCoT prompting is robust to examples and achieves substantial improvements.},
	urldate = {2024-02-03},
	author = {Li, Jia and Li, Ge and Li, Yongmin and Jin, Zhi},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 3},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, Software Engineering (cs.SE)},
	annote = {Other
arXiv admin note: text overlap with arXiv:2303.17780},
}

@misc{press_measuring_2023,
	title = {Measuring and {Narrowing} the {Compositionality} {Gap} in {Language} {Models}},
	url = {http://arxiv.org/abs/2210.03350},
	doi = {10.48550/arXiv.2210.03350},
	abstract = {We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Press, Ofir and Zhang, Muru and Min, Sewon and Schmidt, Ludwig and Smith, Noah A. and Lewis, Mike},
	month = oct,
	year = {2023},
	note = {arXiv:2210.03350 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: To appear at Findings of EMNLP 2023},
	file = {arXiv Fulltext PDF:/Users/chrissoria/Zotero/storage/AC839VXH/Press et al. - 2023 - Measuring and Narrowing the Compositionality Gap i.pdf:application/pdf;arXiv.org Snapshot:/Users/chrissoria/Zotero/storage/NHAK6UMY/2210.html:text/html},
}

@misc{madaan_self-refine_2023,
	title = {Self-{Refine}: {Iterative} {Refinement} with {Self}-{Feedback}},
	shorttitle = {Self-{Refine}},
	url = {http://arxiv.org/abs/2303.17651},
	doi = {10.48550/arXiv.2303.17651},
	abstract = {Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by {\textasciitilde}20\% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},
	month = may,
	year = {2023},
	note = {arXiv:2303.17651 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Code, data, and demo at https://selfrefine.info/},
	file = {arXiv Fulltext PDF:/Users/chrissoria/Zotero/storage/VY8BRK7B/Madaan et al. - 2023 - Self-Refine Iterative Refinement with Self-Feedba.pdf:application/pdf;arXiv.org Snapshot:/Users/chrissoria/Zotero/storage/Q8BVF8HC/2303.html:text/html},
}

@misc{dhuliawala_chain--verification_2023,
	title = {Chain-of-{Verification} {Reduces} {Hallucination} in {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.11495},
	doi = {10.48550/arXiv.2309.11495},
	abstract = {Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason},
	month = sep,
	year = {2023},
	note = {arXiv:2309.11495 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/chrissoria/Zotero/storage/R3TW2QZJ/Dhuliawala et al. - 2023 - Chain-of-Verification Reduces Hallucination in Lar.pdf:application/pdf;arXiv.org Snapshot:/Users/chrissoria/Zotero/storage/3WJSM5T7/2309.html:text/html},
}
