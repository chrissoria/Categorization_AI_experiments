{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c77629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate #where we change the AI \"personality\"\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129c22b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a25a7471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrissoria/anaconda3/envs/AI/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use langchain_openai.ChatOpenAI instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "#openai.api_key = os.getenv(\"api.txt\")\n",
    "COMPLETIONS_MODEL = \"text-davinci-002\"\n",
    "BETTER_COMPLETIONS_MODEL = \"text-davinci-003\" #for my purposes, this is better\n",
    "LONG_MODEL = \"gpt-3.5-turbo-16k\"\n",
    "REGULAR_MODEL = \"gpt-3.5-turbo\"\n",
    "GPT_4 = \"gpt-4-1106-preview\"\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0,\n",
    "                  openai_api_key = API_KEY,\n",
    "                  verbose=True,\n",
    "                  model_name=GPT_4) #depending on how big of a task\n",
    "\n",
    "#below, we give the AI a \"personality\"\n",
    "template = \"\"\"The following is a conversation between a human data scientist and an AI who specializes in data categorization. The AI is direct and provides concise responses. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Data Scientist: {input}\n",
    "AI:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b96d96b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chrissoria/Documents/Research/Categorization_AI_experiments\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/Users/chrissoria/Documents/Research/Categorization_AI_experiments')\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a7b7a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a19i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relocated back to east coast - closer to my sons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>move in together with my partner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>out of living with my friends, and into living...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>to take a new job in new york city (both becau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wanted to live in my own place outside my pare...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                a19i\n",
       "0   relocated back to east coast - closer to my sons\n",
       "1                   move in together with my partner\n",
       "2  out of living with my friends, and into living...\n",
       "3  to take a new job in new york city (both becau...\n",
       "4  wanted to live in my own place outside my pare..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survey_participant_input = \"a19i\" #enter column name here\n",
    "\n",
    "UCNets = pd.read_excel(\"/Users/chrissoria/Documents/Research/UCNets_Classification/data/Raw_Cond_for_Coding_all_waves.xlsx\", engine='openpyxl',sheet_name=\"JOINT_DATA\",usecols=[survey_participant_input])\n",
    "UCNets = UCNets[survey_participant_input].dropna().unique()  # Drop NaN values and get unique elements\n",
    "\n",
    "survey_participant_responses = '; '.join(str(item) for item in UCNets) #what we will feed to the model\n",
    "\n",
    "UCNets = pd.DataFrame(UCNets, columns=[survey_participant_input])\n",
    "UCNets[survey_participant_input] = UCNets[survey_participant_input].astype(str).str.lower()\n",
    "UCNets[survey_participant_input] = UCNets[survey_participant_input].str.strip()\n",
    "UCNets = UCNets[UCNets[survey_participant_input] != ''].reset_index(drop=True) #trimming all empty rows\n",
    "\n",
    "UCNets = UCNets.iloc[:200]\n",
    "\n",
    "UCNets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b77e00c",
   "metadata": {},
   "source": [
    "Here, I'm trying to \"force\" the model to \"think\" in steps by first A. trying to process the response into its own words and B. having it interact with that object. That is, instead of all steps being given at once, I'm having it think in steps. \n",
    "\n",
    "This time, I will have it think in a \"chain,\" where I will have it output a response and then feed that response back to it in a seperate prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d645134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrissoria/anaconda3/envs/AI/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/Users/chrissoria/anaconda3/envs/AI/lib/python3.8/site-packages/langchain_core/_api/deprecation.py:189: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This respondent moved because they wanted to be closer to their workplace in San Francisco and to afford a condo in Richmond. The move was influenced by both the respondent's and their husband's work locations.\n",
      "{\n",
      "\"1\": \"0\",\n",
      "\"2\": \"0\",\n",
      "\"3\": \"1\",\n",
      "\"4\": \"1\",\n",
      "\"5\": \"1\",\n",
      "\"6\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "survey_input = UCNets['a19i'][47] \n",
    "\n",
    "category = \"\"\"1. to start living with or to stay with partner/spouse. \\\n",
    "2. relationship change (divorce, breakup, etc). \\\n",
    "3. the respondent had a job/school/career change, including transferred. \\\n",
    "4. the respondent's partner's job/school/career change, including transferred. \\\n",
    "5. financial reasons (rent is too expensive, pay raise, etc).\n",
    "6. related to housing features, such as a bigger or smaller yard or house\"\"\"\n",
    "\n",
    "example_JSON = \"\"\"{ \\\n",
    "\"1\": \"0\", \\\n",
    "\"2\": \"1\", \\\n",
    "\"3\": \"0\", \\\n",
    "\"4\": 1, \\\n",
    "\"5\": \"0\", \\\n",
    "\"6\": \"0\"\n",
    "}\"\"\"\n",
    "\n",
    "caveat_1 =\"\"\"Be specific about how the move might've been because of a partner.\"\"\"\n",
    "\n",
    "template_string1 = \"\"\"A survey respondent was asked, \"Why did you move?\" \\\n",
    "They responded with: \"{OBJECT}\" \\\n",
    "First, filter out anything in this response that doesn't answer the question, \"Why did you move?\" \\\n",
    "Second, explain to me all of the reasons why they moved. \\\n",
    "{CAVEAT}\n",
    "Format your response in a as few words as possible starting with the words, 'This respondent moved because...'\"\"\"\n",
    "\n",
    "prompt_template1 = ChatPromptTemplate.from_template(template_string1)\n",
    "prompt_template1.messages[0].prompt #this will show us our prompt template\n",
    "\n",
    "GPT_Responses1 = prompt_template1.format_messages(\n",
    "                    OBJECT=survey_input,\n",
    "                    CAVEAT=caveat_1)\n",
    "\n",
    "TEST1 = chat(GPT_Responses1)\n",
    "TEST1 = TEST1.content\n",
    "\n",
    "template_string2 = \"\"\"A survey respondent was asked, \"Why did you move?\" \\\n",
    "\"{OBJECT}\" \\\n",
    "Please determine how many of the following reasons for moving they provide from this list: \\\n",
    "{CATEGORY} \\\n",
    "Next, provide your answer as a 1 if yes and a 0 if no in JSON format \\\n",
    "Please do not provide any other text beyond the JSON. \\\n",
    "Here's an example of how you should format your response: \\\n",
    "{EXAMPLE}\"\"\"\n",
    "\n",
    "prompt_template2 = ChatPromptTemplate.from_template(template_string2)\n",
    "prompt_template2.messages[0].prompt #this will show us our prompt template\n",
    "\n",
    "GPT_Responses2 = prompt_template2.format_messages(\n",
    "                    OBJECT=TEST1,\n",
    "                    CATEGORY=category,\n",
    "                    EXAMPLE=example_JSON)\n",
    "\n",
    "TEST2 = chat(GPT_Responses2)\n",
    "TEST2 = TEST2.content\n",
    "print(TEST1)\n",
    "print(TEST2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b94a5b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the last interview i was in walnut creek, we moved to alameda to be closer to the city of san francisco, which is where my husband and i work and then we moved to richmond this past january (2017) so that we could afford buying a condo.\n"
     ]
    }
   ],
   "source": [
    "print(UCNets['a19i'][47])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ef4c12d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'CAVEAT'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(UCNets)):\n\u001b[1;32m      2\u001b[0m     survey_input \u001b[38;5;241m=\u001b[39m UCNets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma19i\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\n\u001b[0;32m----> 3\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_template1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mOBJECT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msurvey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     response \u001b[38;5;241m=\u001b[39m chat(response)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Assuming a successful attempt means a non-empty response\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/langchain_core/prompts/chat.py:614\u001b[0m, in \u001b[0;36mChatPromptTemplate.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend([message_template])\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    612\u001b[0m     message_template, (BaseMessagePromptTemplate, BaseChatPromptTemplate)\n\u001b[1;32m    613\u001b[0m ):\n\u001b[0;32m--> 614\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[43mmessage_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    615\u001b[0m     result\u001b[38;5;241m.\u001b[39mextend(message)\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/langchain_core/prompts/chat.py:228\u001b[0m, in \u001b[0;36mBaseStringMessagePromptTemplate.format_messages\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_messages\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BaseMessage]:\n\u001b[1;32m    220\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format messages from kwargs.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m        List of BaseMessages.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/langchain_core/prompts/chat.py:284\u001b[0m, in \u001b[0;36mHumanMessagePromptTemplate.format\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format the prompt template.\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m        Formatted message.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m HumanMessage(content\u001b[38;5;241m=\u001b[39mtext, additional_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/langchain_core/prompts/prompt.py:132\u001b[0m, in \u001b[0;36mPromptTemplate.format\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Format the prompt with the inputs.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m        prompt.format(variable1=\"foo\")\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_partial_and_user_variables(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDEFAULT_FORMATTER_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/string.py:163\u001b[0m, in \u001b[0;36mFormatter.format\u001b[0;34m(self, format_string, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/site-packages/langchain_core/utils/formatting.py:18\u001b[0m, in \u001b[0;36mStrictFormatter.vformat\u001b[0;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo arguments should be provided, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meverything should be passed as keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m     )\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/string.py:167\u001b[0m, in \u001b[0;36mFormatter.vformat\u001b[0;34m(self, format_string, args, kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_string, args, kwargs):\n\u001b[1;32m    166\u001b[0m     used_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m--> 167\u001b[0m     result, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mused_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_unused_args(used_args, args, kwargs)\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/string.py:207\u001b[0m, in \u001b[0;36mFormatter._vformat\u001b[0;34m(self, format_string, args, kwargs, used_args, recursion_depth, auto_arg_index)\u001b[0m\n\u001b[1;32m    203\u001b[0m     auto_arg_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# given the field_name, find the object it references\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m#  and the argument it came from\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m obj, arg_used \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfield_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m used_args\u001b[38;5;241m.\u001b[39madd(arg_used)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# do any conversion on the resulting object\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/string.py:272\u001b[0m, in \u001b[0;36mFormatter.get_field\u001b[0;34m(self, field_name, args, kwargs)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_field\u001b[39m(\u001b[38;5;28mself\u001b[39m, field_name, args, kwargs):\n\u001b[1;32m    270\u001b[0m     first, rest \u001b[38;5;241m=\u001b[39m _string\u001b[38;5;241m.\u001b[39mformatter_field_name_split(field_name)\n\u001b[0;32m--> 272\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;66;03m# loop through the rest of the field_name, doing\u001b[39;00m\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;66;03m#  getattr or getitem as needed\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m is_attr, i \u001b[38;5;129;01min\u001b[39;00m rest:\n",
      "File \u001b[0;32m~/anaconda3/envs/AI/lib/python3.8/string.py:229\u001b[0m, in \u001b[0;36mFormatter.get_value\u001b[0;34m(self, key, args, kwargs)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m args[key]\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'CAVEAT'"
     ]
    }
   ],
   "source": [
    "for i in range(len(UCNets)):\n",
    "    survey_input = UCNets['a19i'][i]\n",
    "    response = prompt_template1.format_messages(\n",
    "                    OBJECT=survey_input)\n",
    "\n",
    "    response = chat(response)\n",
    "    \n",
    "    # Assuming a successful attempt means a non-empty response\n",
    "    if response.content:\n",
    "        print(f\"Successful attempt for row number for chain 1: {i}\")\n",
    "    \n",
    "    UCNets.at[i, 'Key_Reasons'] = response.content\n",
    "    \n",
    "for i in range(len(UCNets)):\n",
    "    survey_input = UCNets['Key_Reasons'][i]\n",
    "    response = prompt_template2.format_messages(\n",
    "                    OBJECT=survey_input,\n",
    "                    CATEGORY=category,\n",
    "                    EXAMPLE=example_JSON)\n",
    "\n",
    "    response = chat(response)\n",
    "    \n",
    "    # Assuming a successful attempt means a non-empty response\n",
    "    if response.content:\n",
    "        print(f\"Successful attempt for row number for chain 2: {i}\")\n",
    "    \n",
    "    UCNets.at[i, 'JSON'] = response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ad6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "UCNets['JSON'] = UCNets['JSON'].astype(str)\n",
    "\n",
    "def remove_non_digits(text):\n",
    "    return re.sub(r'[^0-9]', '', text)\n",
    "\n",
    "def add_colon_before_comma(text):\n",
    "    new_text = \"\"\n",
    "    for i, char in enumerate(text):\n",
    "        if i < len(text) - 1 and text[i + 1] == ',':\n",
    "            new_text += ':' + char\n",
    "        else:\n",
    "            new_text += char\n",
    "    return new_text\n",
    "\n",
    "def add_colon_before_end(text):\n",
    "    if len(text) > 3:\n",
    "        return text[:-3] + ':\"' + text[-3:]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Apply the function to the 'JSON' column\n",
    "UCNets['JSON_clean'] = UCNets['JSON'].astype(str).apply(remove_non_digits)\n",
    "UCNets['JSON_clean'] = UCNets['JSON_clean'].apply(lambda x: x[-12:]) #= number of categories / 2\n",
    "UCNets['JSON_clean'] = UCNets['JSON_clean'].apply(lambda x: ','.join(x[i:i+2] for i in range(0, len(x), 2)))\n",
    "UCNets['JSON_clean'] = UCNets['JSON_clean'].apply(add_colon_before_comma)\n",
    "UCNets['JSON_clean'] = UCNets['JSON_clean'].apply(lambda x: \"{\" + x + \"}\")\n",
    "UCNets['JSON_clean'] = UCNets['JSON_clean'].apply(lambda x: '\"'.join(x[i:i+1] for i in range(0, len(x))))\n",
    "UCNets['JSON_clean'] = UCNets['JSON_clean'].apply(add_colon_before_end)\n",
    "\n",
    "\n",
    "UCNets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8224e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_backticks(text):\n",
    "    return text.replace('```', '')\n",
    "\n",
    "def remove_json(text):\n",
    "    return text.replace('json', '')\n",
    "\n",
    "UCNets['JSON'] = UCNets['JSON'].apply(remove_backticks)\n",
    "UCNets['JSON'] = UCNets['JSON'].apply(remove_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ce690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#UCNets = pd.read_csv('data/a19i_all_COT_gpt4.csv')\n",
    "\n",
    "normalized_data_list = []\n",
    "error_lines = []\n",
    "\n",
    "for i, json_str in enumerate(UCNets['JSON']):\n",
    "    try:\n",
    "        parsed_obj = json.loads(json_str)\n",
    "        normalized_data_list.append(pd.json_normalize(parsed_obj))\n",
    "    except json.JSONDecodeError:\n",
    "        error_lines.append(i)\n",
    "        continue\n",
    "\n",
    "# Concatenate the normalized data into one DataFrame\n",
    "normalized_data = pd.concat(normalized_data_list, ignore_index=True)\n",
    "\n",
    "error_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfe54e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "UCNets = pd.concat([UCNets, normalized_data], axis=1)\n",
    "UCNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83d9fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "UCNets.to_csv('data/a19i_all_COT_gpt4.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“AI”",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
